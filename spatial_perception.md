# 空間的な認識能力の実現
現実世界に存在する物体の形状、種類、位置、姿勢、速度、加速度など、
空間的な変数を推定する能力に関する技術情報のまとめです。

ここでは、主にユーザは一人として、
ユーザの状態と、部屋にあるものの状態を推定する技術にフォーカスします。

## 関連プロジェクト & プロダクト


## 割と成熟した技術
* mediapipe
* opencv
* open3d
* realsense
* leapmotion
* VRのセンサ(IMU & SLAM)

## リサーチ
### 現状と今後
検出、トラッキング、予測に関する技術がいろんなセンサ構成で行われている。2D検出はかなり成熟。3D検出もできつつある。トラッキング、予測、意図推定はまだまだディープラーニングベースはうまく行っていないようで古典技術を組み合わせたほうが良さそう。

### 文献


## 何に使えるか？
### ユーザ行動認識（室内)
* アシスタント
  * 健康に関するアドバイス
* AI側の自発的なコミュニケーション
  * ユーザの動きに反応して発言する
* ジェスチャコミュニケーション
  * ピースすると、ピースしてくれる?
  * あまり用途がなさそう。

この方向で機械学習ベースでやっていくなら、ユーザの動きはもちろん、
部屋にあるものの検出もする必要があり、更に、動きが何かをラベル付していく必要があるため、
手間がかかる割には、アプリとして良い点が少なそうです。

#### 方向1：ユーザ設定ベース
部屋の中の人の動き検知とアプリ
部屋の地図作成、その地図上のどこに人がいるかを計算するアルゴリズムを作成する。
まずは地図作成して、SLAMでカメラ位置を推定。そこからユーザの顔の位置、手の位置から、位置を推定する。地図にゾーンを設定して、設定とルールベースで3Dモデルを動かす。

##### 想定するアプリ
寝ていた時間の取得、働きすぎ、さぼりすぎのチェック。適度な休憩と労働バランスを取るように。
関節の状態を全て取れたとしても難しい気がする。

#### 方向2: 移動パターンとユーザーとのやり取りで自動で何をしてるか学習させる。
インタラクションによる学習と知識グラフの活用
AI「何してるの？」
ユーザー「〜〜」
知識グラフを使って、
ルールベースのところに落とし込む。

https://developers.google.com/knowledge-graph

#### 方向3: アクション認識のモデルを使う
座ってること立っていることはわかりそう。
座って仕事してることは難しい。パソコンの位置と指を動かしてるの事などわかる必要あり。
立っている座っているという状態と座標値から、ルールベースで作ることはできるかも。

### 障害物回避
* ヒューマノイドロボット

### ジェスチャ操作
手を降ったら自分に近づいてきてくれるなど？

### AR/VR外で手で物理インタラクション
仮想空間でキャラをなでたりする機能をカメラで作る。

### ARの補助
手の上にキャラを乗せるなど。

### VRの補助センサ
人にジャイロセンサをつけて、人の動きを測定することなどできる。

### モーションキャプチャとして利用
