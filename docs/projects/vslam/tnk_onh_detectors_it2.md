# 2st iteration: 主にPoCアプリの検討
## 開発方針（もう少し広く）
### ライブラリとAPI
早い段階で配布して、フィードバックを得ながら改善していく方針で行く。
そのためにアプリに組み込みやすい形式で配布する。

APIの座標はカメラ座標で提供する。また、予め作成した地図上の座標に直す選択肢もユーザに残しておく。単位はUnityなどで扱いやすくするためにメートルにする。

!!! todo
    * HMDとの位置関係をどうやって求める？
    * calibration、地図を合わせてマッピング？(Kinectを参考に)
    * 頭の姿勢・位置を使う？
    * 3D Keypoint detectorの検討

### 配布と改善のサイクル
APIを決めて、基本的なAPIと利用方法のドキュメントをきちんとメンテナンスする。
当面はPythonラッパーのみを提供して、IPCなどプロセス通信で粗結合化されているプロジェクトをターゲットにして普及をすすめる。もしくはC++で活用。

C++利用、Python利用のドキュメントはきちんとメンテ。
リリースノートとかも書いていく??

リリース計画は

* 2D tnk onh detectorのリリース。with tracking graph.
* body tracking with camera.
* body tracking with kinect.
* 3D tnk onh detectorのリリース。

という感じで行くことにします。

!!! todo
    * API docどうする？(Open3d）とりあえずコメントで..

### モジュール分割と依存の整理とその実装
Pythonのモジュールの切り方は次のとおりでいく。

* body_perception: 一般的なFull body trackingの実装
* sexual_object_perception: sexualな物体の検知
* extra_body_perception: 一般的なFull body tracking以上のTracking

の3つのPythonパッケージを提供する。
全てのソースコードは同じリポジトリ(virtual-skinkship-augmenting-mechanism(vsam))という名前
bazel上のWORKSPACEは切らない方針でいく。

* pros
  * WORKSPACEのファイルメンテコストが減る
  * メンテするべきはBUILDファイルのみ
* cons
  * githubリポジトリを分けた場合に
    * 一番簡単な方法がsubmoduleで分けること(BUILDでmediapipeのsoへの依存を描いていた場合)
    * もしくはワークスペースを分けるコストが増える

bazelを使う場合、transitive dependencyがないため、細かい単位でWORKSPACEを切ってしまうと
WORKSPACEのメンテコストが大きく増える。

また、bazelのシェルコマンド実行機能を使えば、
コピーなどもできるので、bazelコマンド一発で、pythonモジュール側の開発に必要な
soファイルの生成ができる。

### 開発手順
* C++でCalculator実装＆Pythonで自動テストを書く
* グラフを書いてGraphRunnerに渡してPythonでテスト

という手順で開発することになる。

### BoundingBoxより精度の高いアウトプット検討

!!! todo
    * 物理エンジンはどの程度出来る？摩擦や変形は？
    * Mesh検出、材質推定とかできるの？
    * 圧力センサの検討

## 地図アノテーションの改善
### SLAMの特性と設定改善


### Azure Kinectを使う
!!! Todo
    * rtabmapで使えるようにPR送る。
    * cartographer onn rosか、ros2を試す。
    * ROS2への移行
    * rtabmapのドキュメント整備を検討
    * cartographerのROSなし利用検討
    * BoundingBoxの調整機能実装

## 2D検出の改善
### 学習データ生成の改善
今回のユースケースでは、BoundingBoxのサイズは出来る限り小さいほうが良い。
今回はBoundingBoxのサイズを出来る限り下げる。cropした点群を使ってピクセルをProjectionして、それを囲むBoundingBoxを生成する方が小さいBoundingBoxを生成できるはずなので、そのアルゴリズムを実装してみたら、BoundingBoxが小さくなった。ただ、まだ何もない領域で

このアルゴリズムは点ごとの処理になるため、処理数は大幅に増加する。
実装時は実行時間に注意しなければならない。Pythonで実行時間を下げるポイントとしては、

次に、オクルージョンの対応を考える。

### オクルージョンの観察と扱い変更
一部だけを囲うBoundingBoxを出すようにはできるはず。

#### 隠れていない場所だけ囲うBoundingBoxを出す


#### 隠れている場所も含めたBoundingBoxを出す
写っていない部分を含んだBoundingBoxを生成出来るようにモデルを変更する方法もある。


#### アノテーションをどうするか

### PoCアプリケーション作り
#### アプリの検討
GIFや動画ファイルと連携するアプリを実装する。

エディタは作らない。

新しい動画に対応するためには、アニメーションと動きを紐付けるための情報を設定する。

* フレームID or 秒で区間を設定する。
* 区間の種類を設定する。（ラベル：出す、入れる）

動画との連動は、次のようにする。
onhへのtnkの入り具合を0.0 - 1.0に正規化する。
この入り具合で表示するフレームの位置を決める。
最終フレームを終了したら、次のフレーム区間に移す。
という感じです。

こういう仕組みでコントロールする場合、
tnkの出し入れのような周期的な運動であれば、
動きの結果、戻るアニメーションになったとしても違和感はない。

!!! todo
    * ラベルではなく、アニメーション方向の設定(入り具合tとフレームの対応方向)と自動再生に汎用化して出来ることは増えるか?→増えない。ラベル名が変わるだけ。UI上の改善のみ
    * 他のアプリで何か使えるか調査する。
    * プロトタイプの作成とコラボ検討
      * 相性が合いそうな人のアニメーションのパラメタ設定
    * 遅延とフレームの少なさが気になる。遅延を減らして改善する。
    * 予測に基づいて、厳密な位置ではなく、大体の方向で出し入れのアニメーションを再生するようにする。

!!! 振り返り
    * Keep
      * 大体直感どおりに作っていい感じに実装できた
      * コードでロジックを整理していったのはヨカッタ
    * Bad/Try
      * はじめからコードでアプリ側のロジックを書けよかった
        * 次からアプリロジックを先に書く
      * アプリは3Dではなく、2Dから試すべきだった。
        * 3Dの連携用アプリの前に2Dのアプリでアルゴリズム側の作り込み、アプリ配布をすすめるほうが早い。
        * 次回から、アプリは出来る限り簡単に。かつデモとしてアルゴリズムの有用性が伝わるものにする。

#### 挿入度合推定
BoundingBox


!!! todo
    * リセットボタンを作る。（トラッキング用のヒストグラムや、トラッキングエラー時のリカバリ）
挿入状態の自動推定

#### トラッキングによるJittering削減
##### トラッキング手法の雑な調査
今回は画像上のトラッキングであるため、

* 画素値を使ったトラッキング
* 画像特徴量を使ったトラッキング
* ベイズフィルタトラッキング

といった選択肢があります。考慮するべきは、
リアルタイムで動かすこと、似た色の画素値が周囲に有る可能性があること、
照明に頑健なDetectionがあるため、トラッキング中の照明は大きく変わらないこと
が上げられます。

画像特徴量を使っていて、計算量が少なそうなものに、
MeanShiftトラッキングがあります。
似た色に引きずられてトラッキングがうまく行かない可能性がありますが、
opencvに実装されていて容易に使えるため、試してみます。

リアルタイム動作できて、画素の影響を受けにくいものに、
パーティクルフィルタがありますが、画素情報があまり活用できません。
ただ、動きをモデル化しやすいので候補として使えるでしょう。
カルマンフィルタでは、周期運動のモデル化が難しいので（前の位置に対して、次の位置が線形ではないため）パーティクルフィルタを使ってトラッキングすることにした。

また、画像特徴点と特徴点移動量推定を使った[Box Tracking](https://developers.googleblog.com/2019/12/object-detection-and-tracking-using-mediapipe.html#more)という手法もあり、(KLM法に近い構造です。)mediapipeに実装されているので試してみる価値は有るかもしれません。

##### 今後の計画
BoundingBoxベースに計算を進めていくと、BoundingBoxの枠のズレにより、
中心点の計算が不安定になる。

##### mediapipeへのモデルの組み込み・グラフの作成
mediapipeで、自分で学習したモデルを実行するためには、
tflite形式に変換する必要がある。tensorflowで学習する際に生成されるcheckpointファイルと学習に使ったconfigファイルがあれば、[mediapipeの手順](https://github.com/google/mediapipe/tree/master/mediapipe/models/object_detection_saved_model)に従えば、
、簡単にtfliteに変換できる。`df89d3e02a41e34581e8065cf5868a9570fa3010`のobject detection apiを使ったが、bazelのリポジトリではなくなっているようで、
bazelコマンドが使えなかった。変更したコマンドは次のとおり。

!!! todo
    * コマンドとconda環境をリポジトリに入れる



### 角度推定
デプス画像と点群からPCAで主軸求めれば角度の推定ができるはずで、
その値に応じて補正をかければ、長さ推定を少しやりやすくなるはず。

## 2D BoundingBoxのみを用いた上限運動の計算検討
* Z軸の補正
  * 物体のサイズのPriorを使って、単位とZ軸の補正ができる？
  * カメラから見た物体の姿勢が変更しない前提で位置がm単位で取れる
* カメラ位置をどうするか
  * カメラに近すぎると物体がカメラの範囲から出る
  * カメラから遠すぎると、位置予測精度が下がる？（実験する。)
  * 1ピクセル当たりの上下移動量が大きくなるのでBoundingBoxの検知誤差に厳しい。
  * データセットが少ないので増やす。

## トラッキングで改善

## Frustum Pointnetの学習
### データセット作成
[公開されているコード](https://github.com/charlesq34/frustum-pointnets)を拡張していく、姿勢が位置自由度なので、3自由度の推定を出来るように変更する。
このリポジトリには2D検出部分は含まれていないため、自前で学習する必要がある。

まずは動作確認。必要なモジュールをインストールする。Kittiデータセットを、このリポジトリで使える形式に変換したものが、pickle形式で公開されているが、これはpython2でpickleされたものでpython3で読み込む場合、[エンコードを指定したりしないといけない。](https://qiita.com/Kodaira_/items/91207a7e092f491fca43)これに注意してソースコードを変更してあとは実行するのみ。

まず、学習データセットの形式を確認する。この形式に合ったpickleファイルを保存することで、
自前データセットの学習機能を実現する。

* id_list: トラック(１インスタンスのIDリスト)
* box2d_list: 2d bounding box(各要素はshape1,4のnd.array)
  * 使われてない。関係なさそう。
* box3d_list: 3d bounding box(各要素はshape(3,8)のnd.array)
  * 順：
* input_list: 入力値。shape(n,4)のnd.array
* label_list: ？？セマンティックセグメンテションの？値？点群と同じshape
* type_list: 文字列。クラス名
* heading_list: yaw
* size_list: bounding boxのサイズ？
* frustum_angle_list: frustumの情報。座標変換のみにしか使われてなさそう。

やるべきことは２つで自前データの加工とモデルの拡張。
1000フレーム２クラスのデータを用意して、2D detectionとPointnetの学習をする。
prepare_dataでtrain,evalはground truth2Dを使って、
frustum内部の点群を抽出している。この処理を真似れば、自前データの処理ができる。
modelsのangleとdatasetクラスを変更すれば角度は増やせるはずです。

!!! todo
    * prepare_data.pyを参考に3D bounding boxデータを元にPointnet学習データを実装する
    * 2D bounding boxを学習する。

### 学習と学習結果
