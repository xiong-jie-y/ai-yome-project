# インタラクティブなふわみくアプリの実装(3D on screen)
## 触れたことと力変化の検出
入力デバイスとしてはディスプレイのような平面の画面を想定する。
タップの強さを力Fに変換して、触っている箇所は画面からカメラ方向へのレイトの交差判定により決定する。力をかけている箇所が一定以上変化した場合（レイが交差するメッシュが変わった場合）に、キャラクターが反応を感じられるようにする。
力の変化のみ感じられるので、撫でるという動作をしないと反応しない。

## どの部位に当たったかを判定
メッシュを当たり判定に使うことで、
（何も手間をかけない方法では）最も正確な当たり判定ができる。
しかし、メッシュは部位ごとにグループ化されているとは限らない。
動かすことを想定したモデルでは、ボーンの設定がされいることが多く、
そのボーンに対して、weightが紐付けられている。
なので、この情報を使ってある程度の部位は判定できる。

欠点としては、

* A. ボーンに体の全ての部位の情報があるわけではない。
* B. アニメーションを制作しやすくするためのみ必要なボーン情報があり、当たり判定のチェックを惑わす。

ので、アプリケーションに関係ない部位を削除したり、必要な部分を追加する必要がある。　

ふわみくモデルでは、大体の部位が設定されていたので、この情報を元に、
当たり判定用のオブジェクトを設定する。
Unityでは、SAColliderBuilderというツールがあり、
今回はこのツールを活用して、メッシュを当たり判定とした。
使い方は、キャラクターの最も親オブジェクト（ボーンを子に持つGameObject)にSABoneClolliderBuilderをつけて、Builderの設定をいじってProcessボタンを押すだけです。
私は次のような設定で書きました。

!!! todo
    * 欠点Aの対策
    * 欠点Bの対策
    * 体の部位の標準化

## 反応を返すアルゴリズムの設計
反応は好感度のように長期的な内部状態と、入力である感覚と文脈（PTO）で決まる。ように思える。POMDPのモデルでいうと、状態が文脈、感覚で、内部状態はモデルが機械学習を通してモデリングしたり、アルゴリズム内部で使われるものです。反応がActionで、表現したいパーソナリティを表現したActionを出すことが今回の問題です。
2020年現在刺激に対する人の反応の大量のデータは収集・整備されておらず、
機械学習で今回のような皮膚への刺激への反応を学習するのは現時点ではかなり難しく、
出来るとしても古典的な手法、ルールベースのアルゴリズムを考える必要があります。

心理学的には、表情が合ったり見てわかりやすく最大１時間程度のみ持続するものをEmotion、
長期間持続して、外見を見てもわからないことがあるものをMoodと[分類](https://www.paulekman.com/blog/mood-vs-emotion-difference-between-mood-emotion/)しているようです。

感情に関しては心理学程度のレベルで分類などがなされているようですが、
感情が生じるプロセスや更に下の生体レベルの仕組みなどは[明らかになっていないことも多い](https://jsai.ixsq.nii.ac.jp/ej/?action=repository_action_common_download&item_id=10483&item_no=1&attribute_id=22&file_no=1)ようです。

まとめるとこの図のような感じで、
接触部位判定により接触判定が行われた後、
好感度と接触部位を入力とした関数により感情が生成され、
その後反応を出力として出す。という設計です。

反応は表現モーフの名前を直接して操作することで実現しますが、
この当たりも今後はある程度共通化して使えるものに変えていく必要がありそうです。

!!! todo
    * 性格の違いをパラメータとして表現したい。
    * 感情出力のモデル化。
    * 表情モーフの標準化計画

## 反応を作って管理する
