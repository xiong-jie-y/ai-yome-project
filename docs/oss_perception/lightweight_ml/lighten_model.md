# 機械学習モデルの最適化
リアルタイム処理をする場合、機械学習モデルの実行速度を下げる必要が有ることもあります。
学習時とは異なり、推論時には、ネットワークの中に簡易化出来る箇所があり、
そういった箇所を省いたりすることで、処理速度やメモリ使用量を減らしたりします。
また、単純にビット数を落とすことも有効で単精度にしたり、１バイトの引数にしたりしても
ある程度の精度は残せる場合があり、高速化するときは検討しても良いと思います。

基本的にはコンバーター、ランタイムごとに

* グラフの最適化アルゴリズムが異なる
* サポートしている演算や最適化が異なる
* ランタイムが異なる

といったことが異なる。それぞれ大きな差はつかないと思うので、
個人で使う場合においては、ONNX Runtimeのような対応しているものが多いものを使うのが良さそうです。

製品作りになると少し要件が強まり、
モデルの最適化アルゴリズムやランタイムはデバイスや用途に最適化されているので、
自分が推論を行いたいデバイスに特化したものを選ぶ感じになると思います。

!!! todo
    選択基準をもっと作り込む

!!! todo
    **OSS参加**
    OSやハードウェアの制約を考えながらパフォーマンスチューニングしたい人とか、性能に関係ない箇所を自動的に探すアルゴリズムを探したい人はこういったOSSに参加するのは楽しいかもしれません。

## TFLiteを使った最適化
[TFLiteはEdgeデバイスの上でモデルを動かすことを想定して作られています。](https://www.tensorflow.org/lite/guide#next_steps)
マルチプラットフォーム、多プログラミング言語対応もそうですが、
パフォーマンス面における特徴としては、

* デバイスに特化した演算単位
* ハードウェアアクセラレーションを活用したKernel
* pre fusedされた活性化とかバイアス
* バイナリサイズが小さい
* [量子化](https://www.tensorflow.org/lite/performance/post_training_quantization#optimization_methods)

デバイスで高速に実行するために、演算が一部の用途に特化した実装になっていたり、
[サポートされている演算に制約](https://www.tensorflow.org/lite/guide/ops_compatibility)があったりするようです。

バイナリサイズが小さいことでモデルのロードが早くなります。

などが上げられる。量子化はCPUで有効で桁を落とすことで並列計算用の機能を活用出来る可能性がある。GPUでも、効果がないわけではないが、そこまで大きな効果があるわけではない。（もともと並列計算するように設計されているため）

## TensorRTを使った最適化
TensorRTはtfliteとは異なり、
データセンター、車、組み込みなど幅広いアプリケーションへの適用を目的としています。
ただ、ハードウェアに関しては、CUDAに限定されそうです。
基本、tfliteと同じで、

* 量子化
* NVIDIAの各種GPU向けの最適化
* LayerやテンソルのFusion

の３つの最適化がされているようです。

## MediaPipeランタイム対応
tfliteとtensorflowは標準で対応されています。
それ以外に関しては以下を参照してください。

### TensorRT
TensorRTは対応しているが、
[TensorRTを使うにはビルド時のフラグを変える必要](https://github.com/google/mediapipe/issues/723)が有るとのことです。

### ONNX
ONNXは非対応
https://github.com/google/mediapipe/issues/111

ONNX Runtimeはinferenceも学習も対応していて、
ONNXモデルはこれを使って動かせば良さそうです。

ONNXはDNNのみでなく、sklearnとかのtraditional mlも動かせるようです。
また、ONNXは対応モデルが多いことが良くて、
色々なモデルoptimizerと組み合わせることもできるようで、
TensorRTで最適化してONNXにして、ONNX Runtimeで動かすこともできるようです。

## Application Example
TensorflowモデルをTensorRTで最適化したのちに、ONNX modelに変換して、
ONNX Runtimeでmediapipe上で実行します。

