# 人体
## # リアルタイム認識に適したMLモデル
リアルタイムの認識パイプラインを作る場合は、
要件に応じた速度や精度を出せるモデルを探す必要があります。
基本、そのようなモデルは論文のAbstractでアピールされていますが、
それ以外にそのようなモデルを探す方法とタスク別にリアルタイム認識に適したモデルを説明します。

## 基本方針


## タスク別のモデル(Desktop GPU)

## タスク別のモデル(Mobile GPU)

## タスク別のモデル(CPU)
# 機械学習モデルの最適化
リアルタイム処理をする場合、機械学習モデルの実行速度を下げる必要が有ることもあります。
学習時とは異なり、推論時には、ネットワークの中に簡易化出来る箇所があり、
そういった箇所を省いたりすることで、処理速度やメモリ使用量を減らしたりします。
また、単純にビット数を落とすことも有効で単精度にしたり、１バイトの引数にしたりしても
ある程度の精度は残せる場合があり、高速化するときは検討しても良いと思います。

基本的にはコンバーター、ランタイムごとに

* グラフの最適化アルゴリズムが異なる
* サポートしている演算や最適化が異なる
* ランタイムが異なる

といったことが異なる。それぞれ大きな差はつかないと思うので、
個人で使う場合においては、ONNX Runtimeのような対応しているものが多いものを使うのが良さそうです。

製品作りになると少し要件が強まり、
モデルの最適化アルゴリズムやランタイムはデバイスや用途に最適化されているので、
自分が推論を行いたいデバイスに特化したものを選ぶ感じになると思います。

!!! todo
    選択基準をもっと作り込む

!!! todo
    **OSS参加**
    OSやハードウェアの制約を考えながらパフォーマンスチューニングしたい人とか、性能に関係ない箇所を自動的に探すアルゴリズムを探したい人はこういったOSSに参加するのは楽しいかもしれません。

## TFLiteを使った最適化
[TFLiteはEdgeデバイスの上でモデルを動かすことを想定して作られています。](https://www.tensorflow.org/lite/guide#next_steps)
マルチプラットフォーム、多プログラミング言語対応もそうですが、
パフォーマンス面における特徴としては、

* デバイスに特化した演算単位
* ハードウェアアクセラレーションを活用したKernel
* pre fusedされた活性化とかバイアス
* バイナリサイズが小さい
* [量子化](https://www.tensorflow.org/lite/performance/post_training_quantization#optimization_methods)

デバイスで高速に実行するために、演算が一部の用途に特化した実装になっていたり、
[サポートされている演算に制約](https://www.tensorflow.org/lite/guide/ops_compatibility)があったりするようです。

バイナリサイズが小さいことでモデルのロードが早くなります。

などが上げられる。量子化はCPUで有効で桁を落とすことで並列計算用の機能を活用出来る可能性がある。GPUでも、効果がないわけではないが、そこまで大きな効果があるわけではない。（もともと並列計算するように設計されているため）

## TensorRTを使った最適化
TensorRTはtfliteとは異なり、
データセンター、車、組み込みなど幅広いアプリケーションへの適用を目的としています。
ただ、ハードウェアに関しては、CUDAに限定されそうです。
基本、tfliteと同じで、

* 量子化
* NVIDIAの各種GPU向けの最適化
* LayerやテンソルのFusion

の３つの最適化がされているようです。

## MediaPipeランタイム対応
tfliteとtensorflowは標準で対応されています。
それ以外に関しては以下を参照してください。

### TensorRT
TensorRTは対応しているが、
[TensorRTを使うにはビルド時のフラグを変える必要](https://github.com/google/mediapipe/issues/723)が有るとのことです。

### ONNX
ONNXは非対応
https://github.com/google/mediapipe/issues/111

ONNX Runtimeはinferenceも学習も対応していて、
ONNXモデルはこれを使って動かせば良さそうです。

ONNXはDNNのみでなく、sklearnとかのtraditional mlも動かせるようです。
また、ONNXは対応モデルが多いことが良くて、
色々なモデルoptimizerと組み合わせることもできるようで、
TensorRTで最適化してONNXにして、ONNX Runtimeで動かすこともできるようです。

## Application Example
TensorflowモデルをTensorRTで最適化したのちに、ONNX modelに変換して、
ONNX Runtimeでmediapipe上で実行します。

# TensorRTを使った最適化
## # TFLiteにコンバート


# 数学# 2Dオブジェクト検出# 3Dオブジェクト検出
## 基礎知識
### Multi Task Learning


## 3D Detectionのアルゴリズム
### # リポジトリの分け方など（開発）
この章では、新しい認識パイプラインを作る際にリポジトリの分け方を説明します。
前提としては、社内などでリポジトリで切り出す前提です。
なので、Bazelなどのビルドシステムの設定方法も説明していきます。

## プロダクト用のWORKSPACEを作る
bazelでは、WORKSPACEファイルを作成することで、一つのリポジトリという単位を定義できる。（これは、Pythonモジュールだとか、アプリ一つだとかそういう単位です。）
このWORKSPACEにその他のライブラリとか他のリポジトリへの依存関係を記述します。

ただ、少し厄介な制約があって、

## # データ型
mediapipeで使われているデータ型の多くは、proto2形式を使って定義されています。proto2形式は、

## Pythonでグラフデータを取得する
# デプス推定
デプス推定とは、

## ステレオデプス推定


## アクティブステレオデプス推定


## 単眼デプス推定
# 
## 並列化のモデル
認識パイプラインを作る場合、スレッドやforkによって、プログラムを並列化すると思いますが、
その場合、自分で並列処理を設計してコーディングする必要があります。
Mediapipeでは、次のような汎用的に使える並列化の仕組みを提供しており、
Calculatorのみ書けば自動的に並列化してくれます。

並列化のモデルはN個のスレッドと優先度つきキューで実行準備が整ったCalculatorのうち、
優先順位が高いものから順に実行されます。

!!! todo
    * スケジューラについて調べる
      * 次のインプットが来て次のグラフが実行されたらどうなる？
        * 元のものは捨てる？
    * アプリに適しているかわかるように
    * この仕組みだと何が保証される？

![](framework_basics_for_impl/scheduler.png)

![](framework_basics_for_impl/calculator_states.png)

## エラー処理
mediapipeのエラー処理はC++の例外システムではなく、独自のフレームワークを使っています。
Statusクラスに成功の有無やエラーの詳細をいれて、それを戻り値で伝搬させるような仕組みです。
これは、[Googleの既存プロダクトで例外があまり使われていないという特殊な事情](https://google.github.io/styleguide/cppguide.html#Exceptions)が関係していて、Googleが公開しているOSSもスタイルガイドの影響を受けているからこうなっています。

基本的にmediapipeの失敗する可能性のあるメソッドの多くは`Status`クラスを返却するようになっていて、
値を返却するメソッドに関しては、`StatusOr<T>`を返却するようになっています。

また、Statusを便利に扱うためのマクロも定義されていますが、クラスでラップする場合など、コンストラクタやデストラクタでは使えないので、
自前のマクロを定義したほうが良さそうです。

このエラーの伝搬の仕方をアプリケーション側でも真似する必要はなくて、あくまで色々なアプリで使うライブラリを作る場合は、
例外を使うよりもこのような自前エラー処理が良いということです。ただ、Statusのチェックをして、失敗時にエラーメッセージを出力しないと、デバッグがやりにくのでその点だけ注意してください。

## データ構造
### 画像
# mediapipeフレームワークの特徴
## 基礎知識
### スレッドとプロセスの扱いの違い
iOS, Android, 

## 概要と解決する課題
[mediapipe](https://arxiv.org/pdf/1906.08172.pdf)はPerceptionパイプラインを

* 複数のプラットフォームやデバイスで動くアプリを
* iterativeに作っていけるようなフレームワーク

です。アプリケーションを作る流れとしては、作者は、

1. 機械学習モデルの選定やアルゴリズムの開発
2. プロトタイプやデモの構築 
3. 品質とリソース消費のバランスを取り
4. 問題を特定して解決する

こととして上げています。私は、アプリとしてリリースすることが目標であれば、
1,3,4に時間がかかると思っています。
1はtfliteモデルやCalculatorの提供によって、
3はCalculatorに処理、グラフにフローを書くことによる設計上の分離により解決し、4はprofilingやvisualizerにより緩和されています。

1に関してはモデルやアルゴリズムをmediapipe上で実装する手間は多少増えます。（CalculatorのI/F対応と、モデルの設定）
3,4はいかにCalculatorの品質とマルチプラットフォーム対応がしっかりしているかという点に左右されます。

個人的にはmediapipeを使ってみた感じ、4の工数はとんとんで、提供されているモデルやアルゴを使うことによる、1工数の削減と3が大きな利点かと思います。4の工数に関しては、Calculatorやサブグラフのコードを辿りにくいのが原因で、扱うCalculatorやSubgraphに詳しくなると変わってくる気もします。
（とにかくアルゴリズムの変更がやりやすい）

ただ、mediapipeは普通のアプリにPerceptionPipelineを追加する以外にもロボットの認識パイプラインの１プロセスとして使うことも検討できそうです。他にも複数人での認識パイプラインの開発を大きく効率化するポテンシャルを持っていると思います。

6年間様々なアプリで成果を上げてきたとのことだが、KPIとか測定したデータは公開されていなかった。ただ、[成功の鍵となったのは、再利用可能なCalculatorとグラフであると述べられていた。](https://arxiv.org/pdf/1906.08172.pdf)

この章では、そういったmediapipeのフレームワークとしての側面を見てみます。

## 利用者別のメリット
### 個人で利用する場合
* メリット
  * トラッキングや検出モデルのCalculatorがあり、それらを組み合わせて高速にデモやアプリを作れる可能性がある
    * また、フレームワークに用意されているCalculatorは、マルチプラットフォームかつ、CPU、GPU両方対応のものが多くて、多くのデバイスで動くものが作れる。
  * CalculatorはThreadPoolで動き、Threadpoolの大きさはデバイスごとに設定されていて、その設定の範囲でベストパフォーンスが出せる。パフォーマンスを出すためのコードを書かなくていいかもしれない。
* デメリット
  * CalculatorはC++でしかかけないので、他の言語で書かれたコードをC++で書き直す必要がある（-2)
  * CalculatorのI/F定義を書く必要がある(-1)

なので、普段からC++でロジックを書いている人なら、mediapipeへの移行コストはそこまで大きくなく、恩恵の方が多くなる可能性がありますが、Pythonで書いていて、速度にも不満がない場合は、特に恩恵はないと思います。

今後の、コミュニティの成長やmediapipeへのCalculatorのコミットにより、
使いやすくなると思います。ただ、公式mediapipeは、メンテナンスするコード量を減らすために、
公式calculatorは絞るはずなので、third_partyの活躍が期待されます。(後述)

!!! todo
    * パフォーマンスチューニングがどの程度必要か体験しておきたい

### 企業で利用する場合


## フローチャートを組み立てるような感覚で認識パイプラインを作る
Mediapipeでは、Calculatorという単位で処理がたくさん用意されていて、
このCalculatorを組み合わせて、データを処理するプログラムをフローで書くことができます。
CalculatorはC++で記述されていて、ある名前、ある型のデータを受け取り、ある型のデータを有る名前として出力する任意の処理です。関数やクラスとは異なり、実行時に１：１で繋がっているわけではなrく、出力を$n>=1$のCalculatorに渡せます。つまり、どちらかというとPub/SubのようなアーキテクチャでCalculatoを組み合わせてプログラムを構築するような感じです。　

このグラフで書く仕組みは、どのようなプログラムにも適しているわけではなく、
60Hzとか90Hzとか一定周期ではいってくるデータを扱うようなもの（語弊があるかもだが、リアルタイム処理）に適しているように思います。
理由としては、mediapipeでは、Calculatorの処理はスレッドに割り当てられていて、
リアルタイムに動作させられるようにスケジューリングされていること、
データ同期やフレームドロップの仕組みが作りやすくなっていること、などが挙げられます。

OSSや企業内での可能性として、
Calculatorを共有する仕組みと、グラフ書いてビルドできるクラウド環境があれば、
企業やOSSで他人が作ったCalculatorやGraphを組み合わせて、
素早く認識パイプラインを作る。といったことが可能になりそうです。
Calculatorもグラフも認識パイプラインに特化していて、
githubでコードリポジトリを共有することよりも、１段階上の共有になりそうです。

また、ロボットのコントローラーなどもCalculatorとして実装して、この枠組みの中で作るようなこともできますがそこまでやる人がここまでなるイメージはない。なぜなら、ROSのコミュニティやROSパッケージの多様性、量、品質に今から追いつけるとは思えないし、mediapipeへの移行メリットも少ないからです。

!!! todo
    * ストリーミング処理は実装できるか？[ペーパーで関連したものとして上げられてリウ。](https://arxiv.org/pdf/1906.08172.pdf)

## Calculatorをどう実装して共有するか?
CalculatorとGraphが増えていくと、mediapipeで作れるものが格段に増える。これらをどのように共有するか考える。
ここでは例えば、3D認識やARの機能をmediapipeを使って、mediapipeのリポジトリとは独立した形でメンテ・提供する方法を考える。SLAMの実装も検討する。

例えば、RGBDカメラを使って、ある物体の3D Bounding Boxを検知してトラッキングするFlowを作るとしよう。

* 3D Bounding Boxのデータ構造(protobuf message?)
* Calculator (3D Bounding Box検知、トラッキング)
* グラフ
* グラフ実行スクリプト

の４つが必要になります。これらの４つを作り、ビルドの依存関係を書いていくことになります。
リポジトリとして、mediapipeを外部パッケージとしてもつリポジトリを作り、
それぞれから、mediapipeのCalculatorなどにアクセスすれば出来ると思います。

## フレームワークの性能上の意義

!!! todo
    * ThreadPoolあたりの実装と今後の可能性を調査

## その他の特徴・改善点 (WIP)
### third_partyリポジトリの検討
OSSコミュニティやその他のアプリ開発者との協調案としては、
高品質かつ、マルチプラットフォームなCalculatorを豊富にメンテナンスするリポジトリがあれば、結構いろんな人が簡単にPerceptionPipelineを作れる。何らかの機械学習モデル提供リポジトリmodel_zooとかとシームレスに連携出来る仕組みやVisualizerが整えばかなり使いやすくなると思います。


### デバッグ・小さい単位でのテストを容易にしたい
* １フレームごとのテストを容易にしたい。
  * → カメラ、ジャイロなどのセンサデータ保存。再生の仕組み。
* 

### Calculator
* 入力と出力の名前と型がプログラムにより定義される。

### 他のプログラミング言語でCalculatorを実装可能にしたい
ROSではプログラミング言語の選択は自由だが、mediapipeでは基本C++でかく、
Rustなどdynamic library化できる言語であれば使えるはずなので、模索したい。
特にRustでの実装を実現したい。Pythonもプロトタイプ実装に役立つはずです。

### ROSへの組み込み
* ROSの他のコンポーネントの連携検討

!!! todo
    * ROSの他のコンポーネントの連携などユースケースを探す
    * ROSにmediapipeと似たフレームワークがないか調べる


* 含まれている機械学習モデルの情報が実装に必要なレベルで用意されている
  * Calculatorでラップされているが、たまに足りない情報があり、Subgraph, Calculatorコードを読む必要がある


## その他のフレームワークとの比較
### ROSとmediapipeの違い
大きな違いは、ノード間の協調方法にあり、ROSでは、実行の単位であるノードがプロセスであり、
Mediapipeでは実行の単位であるCalculatorがスレッドであるということです。
この２つの違いから次のような相違点が出てきます。

まずは、ノード間の通信の遅延です。
ROSでは、特にシリアライズのオーバーヘッドにプラスして、ROS1では、HTTP＋XMLRPCのオーバーヘッドも追加されるため、全てのノードが同じPCで動作していても、ms単位で遅延が載ってくると想像でき、90FPSとかが要求されるアプリとか、計算リソースが貧弱なコンピュータ上で動かすなら、
シリアライズや通信がないmediapipeが適しているように思えます。
[mediapipeのペーパー](https://arxiv.org/pdf/1906.08172.pdf)にもそのように書かれています。ROS2の設計時の[DDSの調査ドキュメント](https://design.ros2.org/articles/ros_on_dds.html)にはROS1でもnodeletsを使ってshared_ptrをプロセス間でやり取りすることで高速なデータやり取りができるようです。

次にノードの分散処理が可能かどうかという違いがあります。
Mediapipeは、Calculatorは全てスレッドであるため、
フレームワークとしてのサポートはありません。
分散処理をするには、ユーザが明示的に他プロセスと通信する必要があります。
一方で、ROSはノードがプロセスであり、プロセス間の通信はROSがやってくれます。

おそらく、ROSのほうがMediapipeよりも複雑なシステムに適しています。
で、Mediapipeは小型のデバイスやリアルタイム処理が必要なアプリに
適していると思います。また、ノード間のデータのやり取りのオーバーヘッドが同じプログラム内の関数レベルなので、ノードの粒度をROSよりも小さくできます。

!!! todo
    * もう少し詳細に理由を書く。

### OpenCV Graph APIとの違い
Graph APIは画像処理フローをグラフで書いて、
そのグラフを元にopencv側が自動的に最適化してくれるような仕組みのようです。
ただ、全ての処理をGraph APIで書けることは保証されておらず、使いどころが難しそうです。

# キーポイント検出
## 基礎知識
### カメラモデル
カメラモデルの特徴を理解しておくと、CGやCVのアルゴリズムの理解に役立ちます。
Weak Perspective ProjectionとPerspective Projectionがよく使われます。

それぞれの特徴を以下に書きます。

* Orthogonal Projection: XY平面への投影＝単にZ軸を取り除くのみ。[変換行列](https://en.wikipedia.org/wiki/Orthographic_projection)を見るとわかりやすい。
  * CGやコンピュータビジョンで使っているのは見たことがない
* [Weak Perspective Projection: ](https://en.wikipedia.org/wiki/3D_projection)
  * Orthogonal Projection + Scaling by distance from camera
  * カメラからの距離に対して十分に奥行きが小さい物体や、FOVが小さいカメラなら誤差が小さい
  * 微分可能なため機械学習モデルでよく使われる
* Perspective Projection: 
  * ピンホールカメラモデルを投影面を工学中心からセンサとは反対側に焦点f分動かしたものです。f動かすことで、画像の上下が反転しなくて、このモデルで投影した画像は扱いやすい。
  * [Unityなど3Dエンジンの描画に使われます。](https://docs.unity3d.com/ja/2018.4/Manual/PhysicalCameras.html)
* 更に複雑なモデル
  * レンズを考慮したり、光の性質（回折とか、干渉）をシミュレートする。
  * ゲームとかではなく、高度なレンダリングで使われそう。

!!! todo
    * 複雑なモデルを確認
    * Unityでカメラいじってみる
    * Orthogonal Projectionの例

## 手のキーポイント検出
### Overview
mediapipeのモデルを使えばかなり手軽にできます。

このモデルをそのまま使う場合ですが、
手のキーポイントの位置関係がわかるため、
手の形状を入力とするアルゴリズムに使えます。

例えば、ジェスチャー認識のうち、手の形状変化が重要なものはこのモデルで検出できます。
手を振る動作など、手のワールド座標上の動きを使うジェスチャーは、
このモデルのみでは厳しいです。（カメラに並行な動きならある程度は取れそうです)

### Palm Detection


### Hand Landmark Model
Hand Landmark Modelの出力は21x3の3次元点です。
ただし、この三次元点は、(x,y)が画素の位置であり、zがカメラ座標のZ軸に平衡な奥行きだと思っています。
ただ、このzに関してはかなり怪しくて、何度もissueにz軸に関する質問が立っています。
例えば、[これ](https://github.com/google/mediapipe/issues/742)。

このissueが一番しっかりした答えですが、怪しいのが、デプスとカメラ座標のZがごっちゃになっているように思えて、zが工学中心点から点への直線距離か、Z軸に平衡な奥行き化が不明確。
また、zがz_avgへの相対的な値なのか、手首のzへの相対的な値なのかも不明確。
ただ、実装と論文を見る限り、出力値は手首のzへの相対的な値で、
zはカメラ座標のZに沿ったzです。

!!! todo
    zの推定はintrinsic parameterがなくてもできる？

おそらく、モデル的にweak perspective projectionを仮定していて、
合成データセットを作る際はperspective projectionを使っているはずだが、
それだと、どこでweak projectionを仮定しているか不明確

[v0.7.6](https://github.com/google/mediapipe/releases/tag/v0.7.6)でZをnormalizeするオプションがついていますが、このnormalizeでは、おそらく、z軸の値の範囲を、
他の画像範囲でも使える形式で大体0.0-1.0の範囲で表すことを目的としている。
学習時の手のz軸の分散で割れば0.0-1.0の範囲になるはずで、
さらに奥行き方向の単位がピクセルなので、ピクセルで割れば無単位になる。
（また、このxyzをそのまま手の形状として使う場合は、手に対して、weak perspective projectionを仮定していることになる）

例えば、横幅でzを割るというルールがあれば、
画像サイズが異なっていても、アスペクト比が同じであれば、幅をzとxにかければ良い。
元のアスペクト比と、zを何で正規化したかという情報があれば元に戻せる。

角度情報を取り出したい場合は、
元のアスペクト比は重要なので元に戻す必要がある。

weak perspective projectionはカメラと物体の距離に対して、
物体の奥行きが小さくて、カメラのFOVが狭い場合は良い近似になるとのことで、
この手のケースでは問題ないはずです。

ワールド座標のZではなく、手首に対する相対デプスを求めている理由だが、

* Cropされた画像のみでは十分な情報がない。（手のサイズが固定だと仮定すると求められる気もするが。）
  * もともとの画像サイズの大小があるので、周りのものとの相対的な位置関係で求める必要がある
  * カメラの内部パラメータがあれば、3次元空間が画像空間にどういう大きさで射影されるかわかるので、カメラ内部パラメータもセットならOK。
* また、Cropされた画像（ある程度バリエーションが減った画像であれば）、学習が簡単になる

### 手のワールド座標上の三次元位置を推定する
例えば、アプリ内の何かを掴ませたいとかそういうアプリを作る場合、
手の位置を知りたいです。ただ、mediapipeでは手の各キーポイントの手首への相対的な位置のみしかわかりません。

また、mediapipeのhand trackingは推論を行うデバイスで[カメラキャリブレーションなどなしで動くアルゴリズムを提供したいらしく](https://github.com/google/mediapipe/issues/99#issuecomment-531301021)、グローバルなZ座標推定にカメラキャリブレーションが必要な現状では、3次元座標推定を提供しないとのことです。
このHand Landmark Modelでは、手首キーポイントに対する、他のキーポイントの相対的な位置はわかるが、
手のワールド空間における位置はわからない。その値を推定するためには、

* カメラの内部パラメータ
* 手首位置のZ座標の値

を推定して、これらの値で求めたZとx,y座標を元に、逆プロジェクションする必要がある。
例えば、2D->3D Liftingでは、[カメラの内部パラメータが入力として必要です。](https://research.fb.com/wp-content/uploads/2019/05/3D-human-pose-estimation-in-video-with-temporal-convolutions-and-semi-supervised-training.pdf)

嘆願ベースのデプス推定でも、[intrinsic parameterは必要な手法が大半](https://arxiv.org/pdf/2003.06620.pdf)のようだが[、カメラの内部パラメータを推定するネットワークを組み込んで](https://arxiv.org/pdf/1904.04998.pdf)、不要なものもあるようです。[camera intrinsic不要なモデルは公式コードもあり。](https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild)

その他、ThreeDPoseなど、リアルタイムで動く3D Pose Esimationと組み合わせるという方法もあります。
この方法だと、手首と他の体のパーツの相対値も使えるため、情報量が多いと思います。
ただ、ThreeDPoseかなりの謎技術で、Qiitaを見る限り、camera intrinsicは固定値を使っている？
ネットワーク内で推定している？

!!! todo
    * ThreeDPoseEstimationの調査

### Calculatorの説明とグラフ実装

## 顔のキーポイントの検出
### Calculatorの説明とグラフ実装

## その他のキーポイント検出を追加する
### Calculatorの説明とグラフ実装
# Mediapipeを結合する方法
## ダイナミックライブラリを使う
# センサフュージョン
## デプス画像を入力についか
# トラッキング# 認識パイプラインの問題
## 要件の分類
* hard deadline or soft deadline
認識パイプラインはほとんどsoft deadline?
アプリ側でカバーできるはず。

!!! todo
    * hard deadlineの例

* １秒間のFPS
* 平均か?毎回満たすべきか?
* FPSは固定か、〜以上か？

## アプリごとの要件
### ロボティクス
ロボットの移動速度、重さとか停止性能による。

### 人と対話するAI


### 入力インターフェース


## フレームレートの制御
要件より、1秒あたりの実行回数をN回に出来るようにシステムを構築するのが目的です。
ゲームと同じようなフレームレート制御がまず必要だと思います。

## 並列化
## CPUとGPU

## # 他のシステムとの結合
認識パイプラインはアプリケーションの一部であることが多いため、
他のシステムとの結合方法も考えます。

## ロボティクス
### SLAMやNavigationコンポーネント: ROS2

## エージェント
### 3Dエンジン：OpenGL + mmdpipy

### 3Dエンジン：Unity

### 物理エンジン：pybullet
